{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad6b1018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          title  digest  content\n",
      "category                        \n",
      "100        5965    5965     5965\n",
      "101        9782    9782     9782\n",
      "102       33672   33672    33672\n",
      "103       25350   25350    25350\n",
      "104       22127   22127    22127\n",
      "106       10337   10337    10337\n",
      "107       27471   27471    27471\n",
      "108       17942   17942    17942\n",
      "109       24315   24315    24315\n",
      "110       15865   15865    15865\n",
      "112       14773   14773    14773\n",
      "113       15803   15803    15803\n",
      "115       16081   16081    16081\n",
      "116       20513   20513    20513\n",
      "正在分词,请耐心等候......\n",
      "------------分词完成-----------\n",
      "----------------------------------分类结果报告-----------------------------------------\n",
      "分类准确率:0.823097278419668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         100       0.80      0.67      0.73      2955\n",
      "         101       0.75      0.50      0.60      4917\n",
      "         102       0.76      0.90      0.83     16769\n",
      "         103       0.93      0.89      0.91     12793\n",
      "         104       0.81      0.77      0.79     10989\n",
      "         106       0.89      0.85      0.87      5147\n",
      "         107       0.90      0.90      0.90     13675\n",
      "         108       0.82      0.86      0.84      9019\n",
      "         109       0.79      0.86      0.82     12176\n",
      "         110       0.82      0.79      0.80      7997\n",
      "         112       0.72      0.74      0.73      7328\n",
      "         113       0.75      0.71      0.73      7860\n",
      "         115       0.83      0.77      0.80      8090\n",
      "         116       0.90      0.90      0.90     10283\n",
      "\n",
      "    accuracy                           0.82    129998\n",
      "   macro avg       0.82      0.79      0.80    129998\n",
      "weighted avg       0.82      0.82      0.82    129998\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_all_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test,y_predict))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#用保存的all_word统计一下词频\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m words_count \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all_words\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_words\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_words\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m:np\u001b[38;5;241m.\u001b[39msize}) \u001b[38;5;66;03m#groupby就是按词分类\u001b[39;00m\n\u001b[1;32m     42\u001b[0m words_count \u001b[38;5;241m=\u001b[39m words_count\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m],ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#降序\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(words_count\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all_words' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer #词集转换成向量\n",
    "from sklearn.naive_bayes import MultinomialNB #朴素贝叶斯多分类\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim #自然语言处理库\n",
    "from gensim import corpora,models,similarities\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = read_file()\n",
    "    #data_1= content.split(data)\n",
    "    contents_clean,all_words = separate_words(data)\n",
    "    df_train = pd.DataFrame({\"contents_clean\":contents_clean,\"label\":data[\"category\"]})\n",
    "    df_train = shuffle(df_train)\n",
    "    #切分数据集\n",
    "    x_train,x_test,y_train,y_test = train_test_split(df_train[\"contents_clean\"].values,df_train[\"label\"].values,test_size=0.5)\n",
    "    #训练\n",
    "    words_train = format_transform(x_train) \n",
    "    vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,ngram_range=(1, 3),lowercase = False)\n",
    "    vectorizer.fit(words_train)#将清洗过的文章分词转化成朴素贝叶斯的矩阵形式[[],[],[],[]...]\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(vectorizer.transform(words_train), y_train)\n",
    "\n",
    "\n",
    "    #测试\n",
    "    words_test = format_transform(x_test)\n",
    "    score = classifier.score(vectorizer.transform(words_test), y_test)\n",
    "    print(\"----------------------------------分类结果报告-----------------------------------------\")\n",
    "    print(\"分类准确率:\" + str(score))\n",
    "    # 预测结果\n",
    "    y_predict=classifier.predict(vectorizer.transform(words_test))\n",
    "    print(classification_report(y_test,y_predict))\n",
    "    #用保存的all_word统计一下词频\n",
    "    words_count = df_all_words.groupby(by=[\"all_words\"])[\"all_words\"].agg({\"count\":np.size}) #groupby就是按词分类\n",
    "    words_count = words_count.reset_index().sort_values(by=[\"count\"],ascending=False) #降序\n",
    "    print(words_count.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#读取数据源\n",
    "def read_file():\n",
    "    data = pd.read_excel('/Users/apple/Desktop/toutiao.xlsx',names=['category','title','digest','content'])\n",
    "    data=data.dropna()    #直接丢弃包括NAN的整条数据\n",
    "    df = data.groupby('category').count()#展示数据规模\n",
    "    print(df)\n",
    "    return data\n",
    "\n",
    "#jieba分词并去停用词\n",
    "def separate_words(data):\n",
    "    content=data.content.values.tolist()#将content文本内容转换为list格式\n",
    "    #读入停用词表\n",
    "    stopwords = pd.read_csv(\"/Users/apple/Desktop/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8') #list\n",
    "    stopwords = stopwords.stopword.values.tolist()\n",
    "    print(\"正在分词,请耐心等候......\")\n",
    "    contents_clean = []   #存储分完词之后结果\n",
    "    all_words = []\n",
    " \n",
    "    for line in content:\n",
    "        current_segment = jieba.lcut(line) #jieba分词\n",
    "        current_segment = [x.strip() for x in current_segment if x.strip()!=''] #去掉分词后出现的大量空字符串\n",
    "        if len(current_segment) > 1 and current_segment != \"\\r\\n\":\n",
    "            line_clean = []\n",
    "            for word in current_segment:\n",
    "                if word in stopwords:\n",
    "                    continue\n",
    "                line_clean.append(word)\n",
    "                all_words.append(str(word))\n",
    "            contents_clean.append(line_clean)        \n",
    "    print('------------分词完成-----------')\n",
    "    return contents_clean, all_words\n",
    "\n",
    "\n",
    "#开始训练\n",
    "def format_transform(x): #x是数据集（训练集或者测试集）\n",
    "    words =[]\n",
    "    for line_index in range(len(x)):\n",
    "        try:\n",
    "            words.append(\" \".join(x[line_index]))\n",
    "        except:\n",
    "            print(\"数据格式有问题\")\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af295bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
